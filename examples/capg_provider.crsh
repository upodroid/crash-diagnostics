ssh_conf = ssh_config(username=args.username, jump_user=args.username, jump_host=args.jump_host, private_key_path=args.private_key_path)
kube_conf = kube_config(path=args.mc_config)

#list out management and workload cluster nodes
wc_provider=capg_provider(
    workload_cluster=args.cluster_name,
    namespace=args.cluster_ns,
    ssh_config=ssh_conf,
    mgmt_kube_config=kube_conf
)
# This is broken. I need to look in to why.
# nodes = resources(provider=wc_provider)
print("BEFORE IT BREAKS")
print(wc_provider)
print("THIS IS BROKEN")
print(resources(provider=wc_provider))
print("THIS IS NOT BROKEN")
nodes = resources(provider = host_list_provider(hosts=wc_provider.hosts, ssh_config=ssh_conf))
print(nodes)

capture(cmd="sudo df -i", resources=nodes)
capture(cmd="sudo crictl info", resources=nodes)
capture(cmd="df -h /var/lib/containerd", resources=nodes)
capture(cmd="sudo systemctl status kubelet", resources=nodes)
capture(cmd="sudo systemctl status containerd", resources=nodes)
capture(cmd="sudo journalctl -xeu kubelet", resources=nodes)

capture(cmd="sudo cat /var/log/cloud-init-output.log", resources=nodes)
capture(cmd="sudo cat /var/log/cloud-init.log", resources=nodes)

#add code to collect pod info from cluster
set_defaults(kube_config(capi_provider = wc_provider))

pod_ns=["default", "kube-system"]

kube_capture(what="logs", namespaces=pod_ns)
kube_capture(what="objects", kinds=["pods", "services"], namespaces=pod_ns)
kube_capture(what="objects", kinds=["deployments", "replicasets"], groups=["apps"], namespaces=pod_ns)

# archive(output_file="diagnostics.tar.gz", source_paths=[conf.workdir])
